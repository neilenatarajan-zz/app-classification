{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes data from IOS apps in the app store and uses various features in to predict their rating. The features have been formatted, as has the target, articulated below. We then train neural networks to provide various predictions.\n",
    "\n",
    "we run a variety of neural network architectures. The first is a very simple network with eight hidden nodes in one layer, using only linear transformations. The second is a dynamically-sized one layer network using a sigmoid activation function, and the third is a large two layer network using the same function. The final is similar to the one layer network, but making use of the 'dropout' method to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries needed for model\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import sklearn\n",
    "\n",
    "from itertools import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Change the following values as necessary to change the output of the model, or when running on a different computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root data directory\n",
    "data_dir = \"/processing-task\"\n",
    "\n",
    "# Input data\n",
    "data_file = \"IOS_app_data.csv\"\n",
    "\n",
    "# Set seed for random number generator\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set outcome to predict, either \"ver\" or \"tot\"\n",
    "to_predict = \"ver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(to_predict == \"tot\"):\n",
    "    outcomeVal = 'user_rating'\n",
    "if(to_predict == \"ver\"):\n",
    "    outcomeVal = 'user_rating_ver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataset = pd.read_csv(os.path.join(data_dir, data_file), sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data Shape and Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in dataset\n",
    "dataset.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5    1660\n",
       "4.0     871\n",
       "0.0     856\n",
       "5.0     694\n",
       "3.5     378\n",
       "3.0     199\n",
       "2.5     129\n",
       "2.0      85\n",
       "1.0      79\n",
       "1.5      49\n",
       "Name: user_rating_ver, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show different values of user ratings and their frequencies\n",
    "dataset[outcomeVal].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'track_name', 'size_bytes', 'currency', 'price',\n",
       "       'rating_count_tot', 'rating_count_ver', 'user_rating',\n",
       "       'user_rating_ver', 'ver', 'cont_rating', 'prime_genre',\n",
       "       'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic',\n",
       "       'app_desc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show column names in dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our attempt to determine rating we will use the following features:\n",
    "* Free/Paid\n",
    "* Number of Screenshots\n",
    "* Number of Languages\n",
    "* Number of Supporting Devices\n",
    "* Length of Title\n",
    "* Length of Description\n",
    "* Content Rating\n",
    "* Size\n",
    "* Vpp Licensing\n",
    "* Rating Count (for relevant rating)\n",
    "* Genre (broken up by category)\n",
    "* Currency (broken up by category)\n",
    "\n",
    "In order to preserve the integrity of the measurement we will not use the other rating (as data influencing the current version's rating will also influence the total rating, and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['ipadSc_urls.num', 'lang.num','sup_devices.num', 'size_bytes', 'vpp_lic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming ratings to preferred values\n",
    "def rating_transformer(input_rating):\n",
    "    if(0 <= input_rating <= 1.5):\n",
    "        return -1\n",
    "    elif(2.0 <= input_rating <= 3.5):\n",
    "        return 0\n",
    "    elif(4.0 <= input_rating <= 5.0):\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform user_rating and user_rating_ver to preferred values.\n",
    "# Instead of \"high\", \"medium\", and \"low\" we will code as 1, 0, and -1 respectively\n",
    "\n",
    "dataset['user_rating'] = dataset['user_rating'].apply(rating_transformer)\n",
    "dataset['user_rating_ver'] = dataset['user_rating_ver'].apply(rating_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(to_predict == \"tot\"):\n",
    "    columns_to_keep += ['rating_count_tot', 'user_rating']\n",
    "if(to_predict == \"ver\"):\n",
    "    columns_to_keep += ['rating_count_ver', 'user_rating_ver']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming price to paid/free binary\n",
    "def price_transformer(input_price):\n",
    "    if(0 < input_price):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['price'] = dataset['price'].apply(price_transformer)\n",
    "columns_to_keep += ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['track_name'] = dataset['track_name'].apply(lambda desc: len(desc))\n",
    "columns_to_keep += ['track_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['app_desc'] = dataset['app_desc'].apply(lambda desc: len(desc))\n",
    "columns_to_keep += ['app_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['cont_rating'] = dataset['cont_rating'].apply(lambda r: int(''.join(x for x in r if x.isdigit())))\n",
    "columns_to_keep += ['cont_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dummies = pd.get_dummies(dataset['prime_genre'])\n",
    "dataset = pd.concat([dataset, genre_dummies], axis=1)\n",
    "columns_to_keep += np.ndarray.tolist(genre_dummies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.loc[:,columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data Shape and Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3225\n",
       "-1     984\n",
       " 0     791\n",
       "Name: user_rating_ver, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show outcome values\n",
    "dataset[outcomeVal].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 34)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show shape of data\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ipadSc_urls.num',\n",
       " 'lang.num',\n",
       " 'sup_devices.num',\n",
       " 'size_bytes',\n",
       " 'vpp_lic',\n",
       " 'rating_count_ver',\n",
       " 'user_rating_ver',\n",
       " 'price',\n",
       " 'track_name',\n",
       " 'app_desc',\n",
       " 'cont_rating',\n",
       " 'Book',\n",
       " 'Business',\n",
       " 'Catalogs',\n",
       " 'Education',\n",
       " 'Entertainment',\n",
       " 'Finance',\n",
       " 'Food & Drink',\n",
       " 'Games',\n",
       " 'Health & Fitness',\n",
       " 'Lifestyle',\n",
       " 'Medical',\n",
       " 'Music',\n",
       " 'Navigation',\n",
       " 'News',\n",
       " 'Photo & Video',\n",
       " 'Productivity',\n",
       " 'Reference',\n",
       " 'Shopping',\n",
       " 'Social Networking',\n",
       " 'Sports',\n",
       " 'Travel',\n",
       " 'Utilities',\n",
       " 'Weather']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show columns kept in dataset\n",
    "columns_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 34)\n",
      "(750, 34)\n",
      "(1250, 34)\n"
     ]
    }
   ],
   "source": [
    "# Stratified split on outcome\n",
    "# 25% testing\n",
    "# 15% validation\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(dataset, dataset[outcomeVal]):\n",
    "    strat_operating_set = dataset.iloc[train_index]\n",
    "    strat_test_set = dataset.iloc[test_index]\n",
    "    \n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(strat_operating_set, strat_operating_set[outcomeVal]):\n",
    "    strat_train_set = strat_operating_set.iloc[train_index]\n",
    "    strat_val_set = strat_operating_set.iloc[test_index]\n",
    "\n",
    "# Print shape of two datasets\n",
    "print(strat_train_set.shape)\n",
    "print(strat_val_set.shape)\n",
    "print(strat_test_set.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = strat_train_set.loc[:,outcomeVal].values.reshape(-1, 1)\n",
    "train_data = strat_train_set.drop(outcomeVal, axis=1).values\n",
    "val_target = strat_val_set.loc[:,outcomeVal].values.reshape(-1, 1)\n",
    "val_data = strat_val_set.drop(outcomeVal, axis=1).values\n",
    "test_target = strat_test_set.loc[:,outcomeVal].values.reshape(-1, 1)\n",
    "test_data = strat_test_set.drop(outcomeVal, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "train_data = preprocessing.scale(train_data)\n",
    "val_data = preprocessing.scale(val_data)\n",
    "test_data = preprocessing.scale(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modelling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Train Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_func, optimizer, num_epochs=25):\n",
    "    \n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "#         if(epoch%100 == 0):\n",
    "#             print()\n",
    "#             print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "#             print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                inputs = Variable(torch.Tensor(train_data))\n",
    "                labels = Variable(torch.Tensor(train_target))\n",
    "                model.train(True)  # Set model to training mode\n",
    "                \n",
    "            else:\n",
    "                inputs = Variable(torch.Tensor(val_data))\n",
    "                labels = Variable(torch.Tensor(val_target))\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            preds = model(Variable(torch.Tensor(inputs))).data.numpy()\n",
    "            preds = np.squeeze(preds)\n",
    "            corrects = np.count_nonzero(np.abs(np.rint(preds)) == np.int32(labels.data.numpy().ravel()))\n",
    "\n",
    "            epoch_acc = corrects / float(inputs.shape[0])\n",
    "            \n",
    "            # Plot epoch accuracy\n",
    "            \n",
    "#             if(epoch%100 == 0):\n",
    "#                 print('{} Acc: {:.4f}'.format(phase, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "    # Best prediction rate on validation set throughout training\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    # Prediction rate on holdout set using threshold of 0.5\n",
    "    best_model.eval()\n",
    "    pred = best_model(Variable(torch.Tensor(test_data))).data.numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    print('Final holdout Acc: {:4f}'\n",
    "          .format(np.count_nonzero(np.abs(np.rint(pred)) == np.int32(test_target.ravel())) / float(test_data.shape[0])))\n",
    "    \n",
    "    print('-' * 30)\n",
    "    print()\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Networks and Output Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instances of models\n",
    "toynet = nn.Linear(features, 1)\n",
    "\n",
    "onelayernet = nn.Sequential(\n",
    "    nn.Linear(features, features//2), \n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(features//2, 1)\n",
    ")\n",
    "\n",
    "twolayernet = nn.Sequential(\n",
    "    nn.Linear(features, features//2),\n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(features//2, features//6), \n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(features//6, 1)\n",
    ")\n",
    "\n",
    "# One layer neural network with a dropout layer\n",
    "dropoutnet = nn.Sequential(\n",
    "    nn.Linear(features, features//2),\n",
    "    nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(features//2, 1)    \n",
    ")\n",
    "\n",
    "nn_models = [(toynet, 'simple linear transformation'),\n",
    "             (onelayernet , 'dynamically sized one-layer network'),\n",
    "             (twolayernet, 'dynamically sized large two-layer network'),\n",
    "             (dropoutnet, 'one-layer network with a dropout feature')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple linear transformation\n",
      "Best val Acc: 0.560000\n",
      "Final holdout Acc: 0.576800\n",
      "------------------------------\n",
      "\n",
      "dynamically sized one-layer network\n",
      "Best val Acc: 0.645333\n",
      "Final holdout Acc: 0.644800\n",
      "------------------------------\n",
      "\n",
      "dynamically sized large two-layer network\n",
      "Best val Acc: 0.645333\n",
      "Final holdout Acc: 0.644800\n",
      "------------------------------\n",
      "\n",
      "one-layer network with a dropout feature\n",
      "Best val Acc: 0.645333\n",
      "Final holdout Acc: 0.644800\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select optimization criterion for all models\n",
    "# We have chosen to use the standard gradient descent algorithm with MSE loss function\n",
    "loss_func = torch.nn.MSELoss()  \n",
    "l_rate = 0.02 #learning rate \n",
    "mom = 0.9 #momentum\n",
    "num_epochs = 6000\n",
    "\n",
    "for model, m_name in nn_models:\n",
    "    print(m_name)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = l_rate, momentum = mom)\n",
    "    model = train_model(model, loss_func, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
